import os
import time
import json
import requests
from datetime import datetime, timedelta
import schedule
from plyer import notification
import logging
import xml.etree.ElementTree as ET
import re
from urllib.parse import quote
from config import categories,query_mapping, default_config

class ArxivMonitor:
    def __init__(self, config_file="arxiv_config.json"):
        """
        åˆå§‹åŒ–arXivç›‘æ§å™¨
        Args:
            config_file: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.config_file = config_file
        self.config = self.load_config()
        self.setup_logging()
        self.base_url = "http://export.arxiv.org/api/query"
        
    def load_config(self):
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        
        if os.path.exists(self.config_file):
            try:
                with open(self.config_file, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                    config["search_queries"] = default_config["search_queries"]
                # åˆå¹¶é»˜è®¤é…ç½®å’Œç”¨æˆ·é…ç½®
                for key in default_config:
                    if key not in config:
                        config[key] = default_config[key]
                
                # å¦‚æœæ²¡æœ‰query_last_checkå­—æ®µï¼Œåˆ›å»ºä¸€ä¸ªç©ºçš„å­—å…¸
                if "query_last_check" not in config:
                    config["query_last_check"] = {}
                
                return config
            except Exception as e:
                print(f"é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é…ç½®: {e}")
                return default_config
        else:
            # åˆ›å»ºé»˜è®¤é…ç½®æ–‡ä»¶
            with open(self.config_file, 'w', encoding='utf-8') as f:
                json.dump(default_config, f, indent=2, ensure_ascii=False)
            return default_config
    
    def save_config(self):
        """ä¿å­˜é…ç½®æ–‡ä»¶"""
        with open(self.config_file, 'w', encoding='utf-8') as f:
            json.dump(self.config, f, indent=2, ensure_ascii=False)
    
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('arxiv_monitor.log', encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def get_folder_name_for_query(self, query):
        """
        æ ¹æ®æœç´¢æŸ¥è¯¢ç”Ÿæˆæ–‡ä»¶å¤¹åç§°
        
        Args:
            query: æœç´¢æŸ¥è¯¢å­—ç¬¦ä¸²
            
        Returns:
            å®‰å…¨çš„æ–‡ä»¶å¤¹åç§°
        """
        # å¦‚æœæœ‰é¢„å®šä¹‰æ˜ å°„ï¼Œä½¿ç”¨æ˜ å°„
        if query in query_mapping:
            return query_mapping[query]
        
        # å¦åˆ™ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶å¤¹å
        safe_name = re.sub(r'[^\w\s-]', '', query)
        safe_name = re.sub(r'[-\s]+', '_', safe_name)
        safe_name = safe_name[:50].strip('_')
        
        # å¦‚æœæ˜¯ç±»åˆ«æŸ¥è¯¢ï¼Œæ·»åŠ å‰ç¼€
        if query.startswith('cat:'):
            return f"category_{safe_name}"
        else:
            return f"keywords_{safe_name}"
    
    def create_download_directory(self, query=None):
        """
        åˆ›å»ºä¸‹è½½ç›®å½•
        
        Args:
            query: å¯é€‰ï¼Œç‰¹å®šæŸ¥è¯¢çš„ç›®å½•
        """
        base_path = self.config["download_path"]
        
        if not os.path.exists(base_path):
            os.makedirs(base_path)
        
        if query and self.config.get("organize_by_query", True):
            folder_name = self.get_folder_name_for_query(query)
            query_path = os.path.join(base_path, folder_name)
            if not os.path.exists(query_path):
                os.makedirs(query_path)
                self.logger.info(f"åˆ›å»ºæŸ¥è¯¢ç›®å½•: {query_path}")
            return query_path
        
        return base_path
    
    def search_papers_direct_api(self, query, max_results=10):
        """
        ç›´æ¥ä½¿ç”¨arXiv APIæœç´¢è®ºæ–‡
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            max_results: æœ€å¤§ç»“æœæ•°
            
        Returns:
            è®ºæ–‡åˆ—è¡¨
        """
        try:
            # æ„å»ºæŸ¥è¯¢å‚æ•°
            params = {
                'search_query': query,
                'start': 0,
                'max_results': max_results,
                'sortBy': 'submittedDate',
                'sortOrder': 'descending'
            }
            
            # å‘é€è¯·æ±‚
            self.logger.info(f"æ­£åœ¨è¯·æ±‚arXiv API: {query}")
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            response = requests.get(self.base_url, params=params, headers=headers, timeout=30)
            response.raise_for_status()
            
            # è§£æXMLå“åº”
            root = ET.fromstring(response.content)
            
            # å®šä¹‰å‘½åç©ºé—´
            namespaces = {
                'atom': 'http://www.w3.org/2005/Atom',
                'arxiv': 'http://arxiv.org/schemas/atom'
            }
            
            papers = []
            entries = root.findall('atom:entry', namespaces)
            
            for entry in entries:
                try:
                    # æå–è®ºæ–‡ä¿¡æ¯
                    paper_id = entry.find('atom:id', namespaces).text.split('/')[-1].split('v')[0]
                    title = entry.find('atom:title', namespaces).text.strip()
                    summary = entry.find('atom:summary', namespaces).text.strip()
                    
                    # æå–ä½œè€…
                    authors = []
                    for author in entry.findall('atom:author', namespaces):
                        name = author.find('atom:name', namespaces)
                        if name is not None:
                            authors.append(name.text)
                    
                    # æå–å‘å¸ƒæ—¥æœŸ
                    published_str = entry.find('atom:published', namespaces).text
                    published = datetime.fromisoformat(published_str.replace('Z', '+00:00'))
                    
                    # æå–PDFé“¾æ¥
                    pdf_url = ""
                    for link in entry.findall('atom:link', namespaces):
                        if link.get('title') == 'pdf':
                            pdf_url = link.get('href')
                            break
                    
                    # æå–ç±»åˆ«
                    categories = []
                    for category in entry.findall('atom:category', namespaces):
                        term = category.get('term')
                        if term:
                            categories.append(term)
                    
                    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°PDFé“¾æ¥ï¼Œæ„é€ ä¸€ä¸ª
                    if not pdf_url:
                        pdf_url = f"https://arxiv.org/pdf/{paper_id}.pdf"
                    
                    papers.append({
                        'id': paper_id,
                        'title': title,
                        'authors': authors,
                        'summary': summary,
                        'published': published,
                        'pdf_url': pdf_url,
                        'categories': categories,
                        'query': query  # æ·»åŠ æŸ¥è¯¢ä¿¡æ¯
                    })
                    
                except Exception as e:
                    self.logger.warning(f"è§£æè®ºæ–‡æ¡ç›®æ—¶å‡ºé”™: {e}")
                    continue
            
            self.logger.info(f"æˆåŠŸè·å– {len(papers)} ç¯‡è®ºæ–‡")
            return papers
            
        except requests.exceptions.RequestException as e:
            self.logger.error(f"è¯·æ±‚arXiv APIå¤±è´¥: {e}")
            return []
        except ET.ParseError as e:
            self.logger.error(f"è§£æXMLå“åº”å¤±è´¥: {e}")
            return []
        except Exception as e:
            self.logger.error(f"æœç´¢è®ºæ–‡æ—¶å‡ºé”™: {e}")
            return []
    
    def search_papers(self, query, max_results=10):
        """
        æœç´¢è®ºæ–‡ - ä¸»å…¥å£
        """
        return self.search_papers_direct_api(query, max_results)
    
    def download_paper(self, paper):
        """
        ä¸‹è½½è®ºæ–‡PDFåˆ°å¯¹åº”çš„æŸ¥è¯¢æ–‡ä»¶å¤¹
        
        Args:
            paper: è®ºæ–‡ä¿¡æ¯å­—å…¸
            
        Returns:
            ä¸‹è½½æˆåŠŸè¿”å›Trueï¼Œå¦åˆ™è¿”å›False
        """
        try:
            if not paper['pdf_url']:
                self.logger.warning(f"è®ºæ–‡ {paper['id']} æ²¡æœ‰PDFé“¾æ¥")
                return False
            
            # è·å–å¯¹åº”æŸ¥è¯¢çš„ä¸‹è½½ç›®å½•
            download_dir = self.create_download_directory(paper.get('query'))
            
            # åˆ›å»ºå®‰å…¨çš„æ–‡ä»¶å
            safe_title = re.sub(r'[^\w\s-]', '', paper['title'])
            safe_title = re.sub(r'[-\s]+', '-', safe_title)
            safe_title = safe_title[:100].strip('-')
            filename = f"{paper['id']}_{safe_title}.pdf"
            filepath = os.path.join(download_dir, filename)
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
            if os.path.exists(filepath):
                self.logger.info(f"æ–‡ä»¶å·²å­˜åœ¨: {filename}")
                return True
            
            # ä¸‹è½½PDF
            self.logger.info(f"æ­£åœ¨ä¸‹è½½åˆ° {os.path.basename(download_dir)}: {paper['title'][:50]}...")
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            # å°è¯•ä¸‹è½½
            response = requests.get(paper['pdf_url'], headers=headers, timeout=60)
            response.raise_for_status()
            
            # æ£€æŸ¥æ˜¯å¦çœŸçš„æ˜¯PDFæ–‡ä»¶
            if not response.content.startswith(b'%PDF'):
                self.logger.warning(f"ä¸‹è½½çš„æ–‡ä»¶ä¸æ˜¯æœ‰æ•ˆçš„PDF: {paper['id']}")
                return False
            
            with open(filepath, 'wb') as f:
                f.write(response.content)
            
            self.logger.info(f"æˆåŠŸä¸‹è½½: {filename} ({len(response.content)} bytes)")
            return True
            
        except Exception as e:
            self.logger.error(f"ä¸‹è½½è®ºæ–‡å¤±è´¥ {paper['id']}: {e}")
            return False
    
    def filter_new_papers(self, papers, query):
        """
        ç­›é€‰æ–°è®ºæ–‡ï¼ˆæœªä¸‹è½½è¿‡çš„ï¼ŒåŸºäºç‰¹å®šæŸ¥è¯¢çš„æ—¶é—´ï¼‰
        
        Args:
            papers: è®ºæ–‡åˆ—è¡¨
            query: å½“å‰æŸ¥è¯¢
            
        Returns:
            æ–°è®ºæ–‡åˆ—è¡¨
        """
        downloaded_ids = set(self.config["downloaded_papers"])
        new_papers = []
        
        for paper in papers:
            if paper['id'] not in downloaded_ids:
                # å¦‚æœæ˜¯é¦–æ¬¡è¿è¡Œï¼Œè·å–æœ€è¿‘24å°æ—¶çš„è®ºæ–‡
                if self.config.get("first_run", True):
                    cutoff_time = datetime.now() - timedelta(days=1000)
                    if paper['published'].replace(tzinfo=None) > cutoff_time:
                        print(f"å› ä¸ºé¦–æ¬¡è¿è¡Œè·å–åˆ°çš„è®ºæ–‡ï¼ˆæŸ¥è¯¢: {query}ï¼‰")
                        new_papers.append(paper)
                else:
                    # æ£€æŸ¥è¯¥æŸ¥è¯¢çš„ä¸Šæ¬¡æ£€æŸ¥æ—¶é—´
                    query_last_check = self.config["query_last_check"].get(query)
                    if query_last_check:
                        try:
                            last_check = datetime.fromisoformat(query_last_check)
                            if paper['published'].replace(tzinfo=None) > last_check.replace(tzinfo=None):
                                print(f"å› ä¸ºåœ¨æŸ¥è¯¢ '{query}' çš„æ—¶é—´ä¹‹åè·å–åˆ°çš„è®ºæ–‡")
                                new_papers.append(paper)
                        except:
                            # å¦‚æœæ—¥æœŸè§£æå¤±è´¥ï¼Œå°±æ·»åŠ è¿™ç¯‡è®ºæ–‡
                            print(f"æ—¥æœŸè§£æå¤±è´¥ï¼ˆæŸ¥è¯¢: {query}ï¼‰")
                            new_papers.append(paper)
                    else:
                        # å¦‚æœè¯¥æŸ¥è¯¢æ²¡æœ‰æ£€æŸ¥è®°å½•ï¼Œæ·»åŠ è®ºæ–‡
                        print(f"ç›´æ¥æ·»åŠ è®ºæ–‡ï¼ˆæŸ¥è¯¢: {query}ï¼Œæ— å†å²è®°å½•ï¼‰")
                        new_papers.append(paper)
        
        return new_papers
    
    def send_notification(self, title, message):
        """å‘é€ç³»ç»Ÿé€šçŸ¥"""
        try:
            notification.notify(
                title=title,
                message=message,
                timeout=10
            )
        except Exception as e:
            self.logger.error(f"å‘é€é€šçŸ¥å¤±è´¥: {e}")
            # å¦‚æœé€šçŸ¥å¤±è´¥ï¼Œè‡³å°‘åœ¨æ§åˆ¶å°æ˜¾ç¤º
            print(f"\nğŸ”” {title}: {message}")
    
    def test_arxiv_connection(self):
        """æµ‹è¯•arxivè¿æ¥å’ŒAPI"""
        print("ğŸ” æ­£åœ¨æµ‹è¯•arXivè¿æ¥...")
        
        try:
            # æµ‹è¯•ç›´æ¥APIè°ƒç”¨
            test_query = "cat:cs.AI"
            papers = self.search_papers_direct_api(test_query, max_results=2)
            
            if papers:
                print(f"âœ… arXiv APIè¿æ¥æˆåŠŸï¼è·å–åˆ° {len(papers)} ç¯‡è®ºæ–‡")
                print(f"ğŸ“„ æµ‹è¯•è®ºæ–‡: {papers[0]['title'][:80]}...")
                print(f"ğŸ‘¥ ä½œè€…: {', '.join(papers[0]['authors'][:2])}")
                print(f"ğŸ“… å‘å¸ƒ: {papers[0]['published'].strftime('%Y-%m-%d')}")
                return True
            else:
                print("âŒ arXiv APIè¿æ¥å¤±è´¥")
                return False
                
        except Exception as e:
            print(f"âŒ arXivè¿æ¥æµ‹è¯•å¤±è´¥: {e}")
            return False
    
    def show_available_categories(self):
        """æ˜¾ç¤ºå¯ç”¨çš„arXivåˆ†ç±»"""
        
        print("\nğŸ“š arXiv ä¸»è¦åˆ†ç±»ç›®å½•:")
        print("=" * 80)
        
        for main_cat, subcats in categories.items():
            print(f"\nğŸ·ï¸  {main_cat}")
            print("-" * 60)
            for cat_code, cat_name in subcats.items():
                print(f"   {cat_code:<15} {cat_name}")
    
    def check_for_new_papers(self):
        """æ£€æŸ¥æ–°è®ºæ–‡"""
        self.logger.info("å¼€å§‹æ£€æŸ¥æ–°è®ºæ–‡...")
        self.create_download_directory()
        
        # å¦‚æœæ˜¯é¦–æ¬¡è¿è¡Œï¼Œæç¤ºç”¨æˆ·
        if self.config.get("first_run", True):
            print("ğŸ“¢ è¿™æ˜¯é¦–æ¬¡è¿è¡Œï¼Œå°†ä¸‹è½½æœ€è¿‘24å°æ—¶çš„è®ºæ–‡ä½œä¸ºæ¼”ç¤º")
        
        all_new_papers = []
        
        # å¯¹æ¯ä¸ªæœç´¢æŸ¥è¯¢è¿›è¡Œç‹¬ç«‹æ£€æŸ¥
        for query in self.config["search_queries"]:
            self.logger.info(f"æœç´¢æŸ¥è¯¢: {query}")
            papers = self.search_papers(query, self.config["max_results"])
            
            if papers:
                self.logger.info(f"æŸ¥è¯¢ '{query}' æ‰¾åˆ° {len(papers)} ç¯‡è®ºæ–‡")
                new_papers = self.filter_new_papers(papers, query)
                
                if new_papers:
                    self.logger.info(f"å…¶ä¸­ {len(new_papers)} ç¯‡æ˜¯æ–°è®ºæ–‡")
                    all_new_papers.extend(new_papers)
                    
                    # æ›´æ–°è¯¥æŸ¥è¯¢çš„æ£€æŸ¥æ—¶é—´
                    self.config["query_last_check"][query] = datetime.now().isoformat()
                else:
                    self.logger.info("æ²¡æœ‰æ–°è®ºæ–‡")
                    # å³ä½¿æ²¡æœ‰æ–°è®ºæ–‡ï¼Œä¹Ÿæ›´æ–°è¯¥æŸ¥è¯¢çš„æ£€æŸ¥æ—¶é—´
                    self.config["query_last_check"][query] = datetime.now().isoformat()
            else:
                self.logger.warning(f"æŸ¥è¯¢ '{query}' æ²¡æœ‰è¿”å›ç»“æœ")
                # å³ä½¿æ²¡æœ‰è¿”å›ç»“æœï¼Œä¹Ÿæ›´æ–°è¯¥æŸ¥è¯¢çš„æ£€æŸ¥æ—¶é—´
                self.config["query_last_check"][query] = datetime.now().isoformat()
        
        # å»é‡ï¼ˆåŸºäºIDï¼‰
        unique_papers = {}
        for paper in all_new_papers:
            if paper['id'] not in unique_papers:
                unique_papers[paper['id']] = paper
        all_new_papers = list(unique_papers.values())
        
        if all_new_papers:
            print(f"\nğŸ¯ æ‰¾åˆ° {len(all_new_papers)} ç¯‡æ–°è®ºæ–‡ï¼Œå¼€å§‹ä¸‹è½½...")
        
        # æŒ‰æŸ¥è¯¢åˆ†ç»„æ˜¾ç¤ºä¸‹è½½è¿›åº¦
        papers_by_query = {}
        for paper in all_new_papers:
            query = paper.get('query', 'unknown')
            if query not in papers_by_query:
                papers_by_query[query] = []
            papers_by_query[query].append(paper)
        
        # ä¸‹è½½æ–°è®ºæ–‡
        successful_downloads = 0
        total_papers = len(all_new_papers)
        
        for query, papers in papers_by_query.items():
            folder_name = self.get_folder_name_for_query(query)
            print(f"\nğŸ“ æ­£åœ¨ä¸‹è½½åˆ°æ–‡ä»¶å¤¹: {folder_name} ({len(papers)} ç¯‡)")
            
            for i, paper in enumerate(papers, 1):
                global_idx = successful_downloads + i
                print(f"ğŸ“¥ æ€»è¿›åº¦: {global_idx}/{total_papers} | å½“å‰æ–‡ä»¶å¤¹: {i}/{len(papers)} - {paper['title'][:50]}...")
                
                if self.download_paper(paper):
                    self.config["downloaded_papers"].append(paper['id'])
                    successful_downloads += 1
        
        # æ ‡è®°é¦–æ¬¡è¿è¡Œå·²å®Œæˆ
        if self.config.get("first_run", True):
            self.config["first_run"] = False
        
        # ä¿æŒå…¨å±€æ£€æŸ¥æ—¶é—´å…¼å®¹æ€§
        self.config["last_check"] = datetime.now().isoformat()
        self.save_config()
        
        # å‘é€é€šçŸ¥
        if successful_downloads > 0:
            message = f"æˆåŠŸä¸‹è½½äº† {successful_downloads} ç¯‡æ–°è®ºæ–‡!"
            self.send_notification("arXivè®ºæ–‡æ›´æ–°", message)
            self.logger.info(message)
            
            # æ‰“å°è®ºæ–‡ä¿¡æ¯
            print("\n" + "="*60)
            print("ğŸ‰ æ–°ä¸‹è½½çš„è®ºæ–‡æ±‡æ€»")
            print("="*60)
            
            for query, papers in papers_by_query.items():
                downloaded_papers = [p for p in papers if p['id'] in self.config["downloaded_papers"]]
                if downloaded_papers:
                    folder_name = self.get_folder_name_for_query(query)
                    print(f"\nğŸ“ {folder_name} ({len(downloaded_papers)} ç¯‡)")
                    print("-" * 40)
                    
                    for paper in downloaded_papers:
                        print(f"ğŸ“„ {paper['title']}")
                        print(f"ğŸ†” {paper['id']}")
                        print(f"ğŸ‘¥ {', '.join(paper['authors'][:2])}" + 
                              (f" ç­‰ {len(paper['authors'])} äºº" if len(paper['authors']) > 2 else ""))
                        print(f"ğŸ“… {paper['published'].strftime('%Y-%m-%d')}")
                        print()
        else:
            self.logger.info("æ²¡æœ‰æ‰¾åˆ°æ–°è®ºæ–‡")
            print("â„¹ï¸  æ²¡æœ‰æ‰¾åˆ°æ–°è®ºæ–‡")
    
    def toggle_organize_by_query(self):
        """åˆ‡æ¢æ˜¯å¦æŒ‰æŸ¥è¯¢ç»„ç»‡æ–‡ä»¶å¤¹"""
        current = self.config.get("organize_by_query", True)
        self.config["organize_by_query"] = not current
        self.save_config()
        
        status = "å¯ç”¨" if self.config["organize_by_query"] else "ç¦ç”¨"
        print(f"âœ… å·²{status}æŒ‰æœç´¢è¯ç»„ç»‡æ–‡ä»¶å¤¹åŠŸèƒ½")
    
    def reset_downloaded_papers(self):
        """é‡ç½®ä¸‹è½½è®°å½•"""
        self.config["downloaded_papers"] = []
        self.config["last_check"] = None
        self.config["query_last_check"] = {}  # é‡ç½®æ‰€æœ‰æŸ¥è¯¢çš„æ£€æŸ¥æ—¶é—´
        self.config["first_run"] = True
        self.save_config()
        print("âœ… å·²é‡ç½®ä¸‹è½½è®°å½•ï¼Œä¸‹æ¬¡æ£€æŸ¥æ—¶å°†é‡æ–°ä¸‹è½½è®ºæ–‡")
    
    def add_search_query(self, query):
        """æ·»åŠ æœç´¢æŸ¥è¯¢"""
        if query not in self.config["search_queries"]:
            self.config["search_queries"].append(query)
            self.save_config()
            self.logger.info(f"æ·»åŠ æœç´¢æŸ¥è¯¢: {query}")
    
    def remove_search_query(self, query):
        """ç§»é™¤æœç´¢æŸ¥è¯¢"""
        if query in self.config["search_queries"]:
            self.config["search_queries"].remove(query)
            # åŒæ—¶åˆ é™¤è¯¥æŸ¥è¯¢çš„æ£€æŸ¥æ—¶é—´è®°å½•
            if query in self.config["query_last_check"]:
                del self.config["query_last_check"][query]
            self.save_config()
            self.logger.info(f"ç§»é™¤æœç´¢æŸ¥è¯¢: {query}")
    
    def start_monitoring(self):
        """å¼€å§‹ç›‘æ§"""
        self.logger.info("å¼€å§‹arXivè®ºæ–‡ç›‘æ§...")
        print("ğŸš€ å¯åŠ¨arXivè®ºæ–‡ç›‘æ§å™¨...")
        
        # ç«‹å³æ‰§è¡Œä¸€æ¬¡æ£€æŸ¥
        self.check_for_new_papers()
        
        # è®¾ç½®å®šæœŸæ£€æŸ¥
        schedule.every(self.config["check_interval_hours"]).hours.do(self.check_for_new_papers)
        
        print(f"â° ç›‘æ§å·²å¯åŠ¨ï¼Œæ¯ {self.config['check_interval_hours']} å°æ—¶æ£€æŸ¥ä¸€æ¬¡")
        print("ğŸ’¡ æŒ‰ Ctrl+C åœæ­¢ç›‘æ§")
        
        try:
            while True:
                schedule.run_pending()
                time.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡æ˜¯å¦æœ‰å¾…æ‰§è¡Œçš„ä»»åŠ¡
        except KeyboardInterrupt:
            self.logger.info("ç›‘æ§å·²åœæ­¢")
            print("\nğŸ›‘ ç›‘æ§å·²åœæ­¢")

def main():
    """ä¸»å‡½æ•°"""
    print("="*60)
    print("ğŸ“š arXivè®ºæ–‡ç›‘æ§å™¨ v2.1 - æ™ºèƒ½æ–‡ä»¶å¤¹ç®¡ç†ç‰ˆ")
    print("="*60)
    
    monitor = ArxivMonitor()
    
    while True:
        print("\nğŸ¯ è¯·é€‰æ‹©æ“ä½œ:")
        print("1. ğŸš€ å¼€å§‹ç›‘æ§")
        print("2. ğŸ” æ‰‹åŠ¨æ£€æŸ¥ä¸€æ¬¡")
        print("3. ğŸ“‹ æŸ¥çœ‹å½“å‰æœç´¢ä¸»é¢˜")
        print("4. â• æ·»åŠ æœç´¢ä¸»é¢˜")
        print("5. â– åˆ é™¤æœç´¢ä¸»é¢˜")
        print("6. â±ï¸  è®¾ç½®æ£€æŸ¥é—´éš”")
        print("7. ğŸ“Š æŸ¥çœ‹ç»Ÿè®¡ä¿¡æ¯")
        print("8. ğŸ§ª æµ‹è¯•arXivè¿æ¥")
        print("9. ğŸ”„ é‡ç½®ä¸‹è½½è®°å½•")
        print("A. ğŸ“š æŸ¥çœ‹arXivåˆ†ç±»ç›®å½•")
        print("B. ğŸ“ åˆ‡æ¢æ–‡ä»¶å¤¹ç»„ç»‡æ–¹å¼")
        print("0. ğŸšª é€€å‡º")
        
        choice = input("\nè¯·è¾“å…¥é€‰æ‹© (0-9, A-B): ").strip().upper()
        
        if choice == '1':
            monitor.start_monitoring()
        
        elif choice == '2':
            print("ğŸ” æ­£åœ¨æ£€æŸ¥æ–°è®ºæ–‡...")
            monitor.check_for_new_papers()
        
        elif choice == '3':
            print(f"\nğŸ“‹ å½“å‰æœç´¢ä¸»é¢˜ (å…±{len(monitor.config['search_queries'])}ä¸ª):")
            for i, query in enumerate(monitor.config["search_queries"], 1):
                folder_name = monitor.get_folder_name_for_query(query)
                last_check = monitor.config["query_last_check"].get(query, "ä»æœªæ£€æŸ¥")
                if last_check != "ä»æœªæ£€æŸ¥":
                    try:
                        last_check = datetime.fromisoformat(last_check).strftime('%Y-%m-%d %H:%M')
                    except:
                        last_check = "æ—¶é—´æ ¼å¼é”™è¯¯"
                print(f"   {i}. {query:<25} â†’ ğŸ“ {folder_name}")
                print(f"      ä¸Šæ¬¡æ£€æŸ¥: {last_check}")
        
        elif choice == '4':
            print("\nğŸ’¡ æ·»åŠ æœç´¢ä¸»é¢˜:")
            print("ä½ å¯ä»¥è¾“å…¥:")
            print("â€¢ arXivåˆ†ç±» (å¦‚: cat:cs.AI)")
            print("â€¢ å…³é”®è¯ (å¦‚: transformer)")
            print("â€¢ å¤åˆæŸ¥è¯¢ (å¦‚: cat:cs.AI AND deep learning)")
            print("\nå¸¸ç”¨ç¤ºä¾‹:")
            
            examples = [
                ("cat:cs.AI", "äººå·¥æ™ºèƒ½"),
                ("cat:cs.LG", "æœºå™¨å­¦ä¹ "),
                ("cat:cs.CV", "è®¡ç®—æœºè§†è§‰"),
                ("cat:cs.CL", "è®¡ç®—è¯­è¨€å­¦/NLP"),
                ("cat:cs.RO", "æœºå™¨äººå­¦"),
                ("cat:stat.ML", "ç»Ÿè®¡æœºå™¨å­¦ä¹ "),
                ("deep learning", "å…³é”®è¯: æ·±åº¦å­¦ä¹ "),
                ("transformer", "å…³é”®è¯: Transformer"),
                ("diffusion model", "å…³é”®è¯: æ‰©æ•£æ¨¡å‹"),
                ("reinforcement learning", "å…³é”®è¯: å¼ºåŒ–å­¦ä¹ ")
            ]
            
            for query, desc in examples:
                print(f"   â€¢ {query:<25} ({desc})")
            
            query = input("\nè¯·è¾“å…¥è¦æ·»åŠ çš„æœç´¢ä¸»é¢˜: ").strip()
            if query:
                monitor.add_search_query(query)
                folder_name = monitor.get_folder_name_for_query(query)
                print(f"âœ… å·²æ·»åŠ æœç´¢ä¸»é¢˜: {query}")
                print(f"ğŸ“ å°†ä¸‹è½½åˆ°æ–‡ä»¶å¤¹: {folder_name}")
        
        elif choice == '5':
            if not monitor.config["search_queries"]:
                print("âŒ æ²¡æœ‰æœç´¢ä¸»é¢˜å¯åˆ é™¤")
                continue
                
            print(f"\nğŸ“‹ å½“å‰æœç´¢ä¸»é¢˜:")
            for i, query in enumerate(monitor.config["search_queries"], 1):
                print(f"   {i}. {query}")
            
            try:
                index = int(input("\nè¯·è¾“å…¥è¦åˆ é™¤çš„ä¸»é¢˜ç¼–å·: ")) - 1
                if 0 <= index < len(monitor.config["search_queries"]):
                    removed_query = monitor.config["search_queries"].pop(index)
                    # åˆ é™¤å¯¹åº”çš„æ£€æŸ¥æ—¶é—´è®°å½•
                    if removed_query in monitor.config["query_last_check"]:
                        del monitor.config["query_last_check"][removed_query]
                    monitor.save_config()
                    print(f"âœ… å·²åˆ é™¤æœç´¢ä¸»é¢˜: {removed_query}")
                else:
                    print("âŒ æ— æ•ˆçš„ç¼–å·")
            except ValueError:
                print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—")
        
        elif choice == '6':
            try:
                current = monitor.config['check_interval_hours']
                hours = int(input(f"è¯·è¾“å…¥æ£€æŸ¥é—´éš”(å°æ—¶ï¼Œå½“å‰: {current}): "))
                if hours > 0:
                    monitor.config["check_interval_hours"] = hours
                    monitor.save_config()
                    print(f"âœ… æ£€æŸ¥é—´éš”å·²è®¾ç½®ä¸º {hours} å°æ—¶")
                else:
                    print("âŒ è¯·è¾“å…¥å¤§äº0çš„æ•°å­—")
            except ValueError:
                print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—")
        
        elif choice == '7':
            print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
            print(f"   â€¢ æœç´¢ä¸»é¢˜æ•°é‡: {len(monitor.config['search_queries'])}")
            print(f"   â€¢ å·²ä¸‹è½½è®ºæ–‡æ•°: {len(monitor.config['downloaded_papers'])}")
            print(f"   â€¢ ä¸‹è½½è·¯å¾„: {monitor.config['download_path']}")
            print(f"   â€¢ æ£€æŸ¥é—´éš”: {monitor.config['check_interval_hours']} å°æ—¶")
            print(f"   â€¢ é¦–æ¬¡è¿è¡Œ: {'æ˜¯' if monitor.config.get('first_run', True) else 'å¦'}")
            print(f"   â€¢ æ–‡ä»¶å¤¹ç»„ç»‡: {'å¯ç”¨' if monitor.config.get('organize_by_query', True) else 'ç¦ç”¨'}")
            
            # æ˜¾ç¤ºå„æŸ¥è¯¢çš„æ£€æŸ¥æ—¶é—´
            print(f"\nğŸ“… å„æŸ¥è¯¢æ£€æŸ¥æ—¶é—´:")
            for query in monitor.config['search_queries']:
                last_check = monitor.config["query_last_check"].get(query, "ä»æœªæ£€æŸ¥")
                if last_check != "ä»æœªæ£€æŸ¥":
                    try:
                        last_check = datetime.fromisoformat(last_check).strftime('%Y-%m-%d %H:%M')
                    except:
                        last_check = "æ—¶é—´æ ¼å¼é”™è¯¯"
                print(f"   â€¢ {query}: {last_check}")
            
            if monitor.config['last_check']:
                print(f"\n   â€¢ å…¨å±€ä¸Šæ¬¡æ£€æŸ¥: {monitor.config['last_check'][:19]}")
            else:
                print(f"\n   â€¢ å…¨å±€ä¸Šæ¬¡æ£€æŸ¥: ä»æœªæ£€æŸ¥")
            
            # æ˜¾ç¤ºå„æ–‡ä»¶å¤¹ç»Ÿè®¡
            if os.path.exists(monitor.config['download_path']):
                print(f"\nğŸ“ æ–‡ä»¶å¤¹ç»Ÿè®¡:")
                total_size = 0
                total_files = 0
                
                for root, dirs, files in os.walk(monitor.config['download_path']):
                    pdf_files = [f for f in files if f.endswith('.pdf')]
                    if pdf_files:
                        folder_size = sum(os.path.getsize(os.path.join(root, f)) for f in pdf_files)
                        folder_name = os.path.basename(root) if root != monitor.config['download_path'] else 'æ ¹ç›®å½•'
                        print(f"   ğŸ“‚ {folder_name}: {len(pdf_files)} ä¸ªæ–‡ä»¶, {folder_size/1024/1024:.1f} MB")
                        total_size += folder_size
                        total_files += len(pdf_files)
                
                print(f"\n   ğŸ¯ æ€»è®¡: {total_files} ä¸ªPDFæ–‡ä»¶, {total_size/1024/1024:.1f} MB")
        
        elif choice == '8':
            monitor.test_arxiv_connection()
        
        elif choice == '9':
            confirm = input("âš ï¸  ç¡®å®šè¦é‡ç½®ä¸‹è½½è®°å½•å—ï¼Ÿè¿™å°†å¯¼è‡´ä¸‹æ¬¡æ£€æŸ¥æ—¶é‡æ–°ä¸‹è½½è®ºæ–‡ (y/N): ")
            if confirm.lower() in ['y', 'yes']:
                monitor.reset_downloaded_papers()
            else:
                print("âŒ æ“ä½œå·²å–æ¶ˆ")
        
        elif choice == 'A':
            monitor.show_available_categories()
        
        elif choice == 'B':
            current = monitor.config.get("organize_by_query", True)
            print(f"\nğŸ“ å½“å‰æ–‡ä»¶å¤¹ç»„ç»‡æ–¹å¼: {'æŒ‰æœç´¢è¯åˆ†æ–‡ä»¶å¤¹' if current else 'æ‰€æœ‰æ–‡ä»¶åœ¨åŒä¸€æ–‡ä»¶å¤¹'}")
            print("æ˜¯å¦è¦åˆ‡æ¢ç»„ç»‡æ–¹å¼ï¼Ÿ")
            print("â€¢ å¯ç”¨: æ¯ä¸ªæœç´¢è¯çš„è®ºæ–‡ä¸‹è½½åˆ°å¯¹åº”çš„æ–‡ä»¶å¤¹")
            print("â€¢ ç¦ç”¨: æ‰€æœ‰è®ºæ–‡ä¸‹è½½åˆ°åŒä¸€ä¸ªæ–‡ä»¶å¤¹")
            
            confirm = input("ç¡®å®šè¦åˆ‡æ¢å—ï¼Ÿ(y/N): ")
            if confirm.lower() in ['y', 'yes']:
                monitor.toggle_organize_by_query()
            else:
                print("âŒ æ“ä½œå·²å–æ¶ˆ")
        
        elif choice == '0':
            print("ğŸ‘‹ å†è§!")
            break
        
        else:
            print("âŒ æ— æ•ˆçš„é€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥")

if __name__ == "__main__":
    main()