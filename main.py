import os
import time
import json
import requests
from datetime import datetime, timedelta
import schedule
from plyer import notification
import logging
import xml.etree.ElementTree as ET
import re
from urllib.parse import quote

class ArxivMonitor:
    def __init__(self, config_file="arxiv_config.json"):
        """
        åˆå§‹åŒ–arXivç›‘æ§å™¨
        
        Args:
            config_file: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.config_file = config_file
        self.config = self.load_config()
        self.setup_logging()
        self.base_url = "http://export.arxiv.org/api/query"
        
    def load_config(self):
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        default_config = {
            "search_queries": [
                "cat:cs.AI",  # äººå·¥æ™ºèƒ½
                "cat:cs.LG",  # æœºå™¨å­¦ä¹ 
                "cat:cs.CV"   # è®¡ç®—æœºè§†è§‰
            ],
            "max_results": 10,
            "download_path": "./arxiv_papers",
            "check_interval_hours": 6,
            "last_check": None,
            "downloaded_papers": [],
            "first_run": True  # æ ‡è®°æ˜¯å¦ä¸ºé¦–æ¬¡è¿è¡Œ
        }
        
        if os.path.exists(self.config_file):
            try:
                with open(self.config_file, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                # åˆå¹¶é»˜è®¤é…ç½®å’Œç”¨æˆ·é…ç½®
                for key in default_config:
                    if key not in config:
                        config[key] = default_config[key]
                return config
            except Exception as e:
                print(f"é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é…ç½®: {e}")
                return default_config
        else:
            # åˆ›å»ºé»˜è®¤é…ç½®æ–‡ä»¶
            with open(self.config_file, 'w', encoding='utf-8') as f:
                json.dump(default_config, f, indent=2, ensure_ascii=False)
            return default_config
    
    def save_config(self):
        """ä¿å­˜é…ç½®æ–‡ä»¶"""
        with open(self.config_file, 'w', encoding='utf-8') as f:
            json.dump(self.config, f, indent=2, ensure_ascii=False)
    
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('arxiv_monitor.log', encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def create_download_directory(self):
        """åˆ›å»ºä¸‹è½½ç›®å½•"""
        if not os.path.exists(self.config["download_path"]):
            os.makedirs(self.config["download_path"])
    
    def search_papers_direct_api(self, query, max_results=10):
        """
        ç›´æ¥ä½¿ç”¨arXiv APIæœç´¢è®ºæ–‡
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            max_results: æœ€å¤§ç»“æœæ•°
            
        Returns:
            è®ºæ–‡åˆ—è¡¨
        """
        try:
            # æ„å»ºæŸ¥è¯¢å‚æ•°
            params = {
                'search_query': query,
                'start': 0,
                'max_results': max_results,
                'sortBy': 'submittedDate',
                'sortOrder': 'descending'
            }
            
            # å‘é€è¯·æ±‚
            self.logger.info(f"æ­£åœ¨è¯·æ±‚arXiv API: {query}")
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            response = requests.get(self.base_url, params=params, headers=headers, timeout=30)
            response.raise_for_status()
            
            # è§£æXMLå“åº”
            root = ET.fromstring(response.content)
            
            # å®šä¹‰å‘½åç©ºé—´
            namespaces = {
                'atom': 'http://www.w3.org/2005/Atom',
                'arxiv': 'http://arxiv.org/schemas/atom'
            }
            
            papers = []
            entries = root.findall('atom:entry', namespaces)
            
            for entry in entries:
                try:
                    # æå–è®ºæ–‡ä¿¡æ¯
                    paper_id = entry.find('atom:id', namespaces).text.split('/')[-1].split('v')[0]
                    title = entry.find('atom:title', namespaces).text.strip()
                    summary = entry.find('atom:summary', namespaces).text.strip()
                    
                    # æå–ä½œè€…
                    authors = []
                    for author in entry.findall('atom:author', namespaces):
                        name = author.find('atom:name', namespaces)
                        if name is not None:
                            authors.append(name.text)
                    
                    # æå–å‘å¸ƒæ—¥æœŸ
                    published_str = entry.find('atom:published', namespaces).text
                    published = datetime.fromisoformat(published_str.replace('Z', '+00:00'))
                    
                    # æå–PDFé“¾æ¥
                    pdf_url = ""
                    for link in entry.findall('atom:link', namespaces):
                        if link.get('title') == 'pdf':
                            pdf_url = link.get('href')
                            break
                    
                    # æå–ç±»åˆ«
                    categories = []
                    for category in entry.findall('atom:category', namespaces):
                        term = category.get('term')
                        if term:
                            categories.append(term)
                    
                    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°PDFé“¾æ¥ï¼Œæ„é€ ä¸€ä¸ª
                    if not pdf_url:
                        pdf_url = f"https://arxiv.org/pdf/{paper_id}.pdf"
                    
                    papers.append({
                        'id': paper_id,
                        'title': title,
                        'authors': authors,
                        'summary': summary,
                        'published': published,
                        'pdf_url': pdf_url,
                        'categories': categories
                    })
                    
                except Exception as e:
                    self.logger.warning(f"è§£æè®ºæ–‡æ¡ç›®æ—¶å‡ºé”™: {e}")
                    continue
            
            self.logger.info(f"æˆåŠŸè·å– {len(papers)} ç¯‡è®ºæ–‡")
            return papers
            
        except requests.exceptions.RequestException as e:
            self.logger.error(f"è¯·æ±‚arXiv APIå¤±è´¥: {e}")
            return []
        except ET.ParseError as e:
            self.logger.error(f"è§£æXMLå“åº”å¤±è´¥: {e}")
            return []
        except Exception as e:
            self.logger.error(f"æœç´¢è®ºæ–‡æ—¶å‡ºé”™: {e}")
            return []
    
    def search_papers(self, query, max_results=10):
        """
        æœç´¢è®ºæ–‡ - ä¸»å…¥å£
        """
        return self.search_papers_direct_api(query, max_results)
    
    def download_paper(self, paper):
        """
        ä¸‹è½½è®ºæ–‡PDF
        
        Args:
            paper: è®ºæ–‡ä¿¡æ¯å­—å…¸
            
        Returns:
            ä¸‹è½½æˆåŠŸè¿”å›Trueï¼Œå¦åˆ™è¿”å›False
        """
        try:
            if not paper['pdf_url']:
                self.logger.warning(f"è®ºæ–‡ {paper['id']} æ²¡æœ‰PDFé“¾æ¥")
                return False
                
            # åˆ›å»ºå®‰å…¨çš„æ–‡ä»¶å
            safe_title = re.sub(r'[^\w\s-]', '', paper['title'])
            safe_title = re.sub(r'[-\s]+', '-', safe_title)
            safe_title = safe_title[:100].strip('-')
            filename = f"{paper['id']}_{safe_title}.pdf"
            filepath = os.path.join(self.config["download_path"], filename)
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
            if os.path.exists(filepath):
                self.logger.info(f"æ–‡ä»¶å·²å­˜åœ¨: {filename}")
                return True
            
            # ä¸‹è½½PDF
            self.logger.info(f"æ­£åœ¨ä¸‹è½½: {paper['title'][:50]}...")
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            # å°è¯•ä¸‹è½½
            response = requests.get(paper['pdf_url'], headers=headers, timeout=60)
            response.raise_for_status()
            
            # æ£€æŸ¥æ˜¯å¦çœŸçš„æ˜¯PDFæ–‡ä»¶
            if not response.content.startswith(b'%PDF'):
                self.logger.warning(f"ä¸‹è½½çš„æ–‡ä»¶ä¸æ˜¯æœ‰æ•ˆçš„PDF: {paper['id']}")
                return False
            
            with open(filepath, 'wb') as f:
                f.write(response.content)
            
            self.logger.info(f"æˆåŠŸä¸‹è½½: {filename} ({len(response.content)} bytes)")
            return True
            
        except Exception as e:
            self.logger.error(f"ä¸‹è½½è®ºæ–‡å¤±è´¥ {paper['id']}: {e}")
            return False
    
    def filter_new_papers(self, papers):
        """
        ç­›é€‰æ–°è®ºæ–‡ï¼ˆæœªä¸‹è½½è¿‡çš„ï¼‰
        
        Args:
            papers: è®ºæ–‡åˆ—è¡¨
            
        Returns:
            æ–°è®ºæ–‡åˆ—è¡¨
        """
        downloaded_ids = set(self.config["downloaded_papers"])
        new_papers = []
        
        for paper in papers:
            if paper['id'] not in downloaded_ids:
                # å¦‚æœæ˜¯é¦–æ¬¡è¿è¡Œï¼Œè·å–æœ€è¿‘24å°æ—¶çš„è®ºæ–‡
                if self.config.get("first_run", True):
                    cutoff_time = datetime.now() - timedelta(days=1)
                    if paper['published'].replace(tzinfo=None) > cutoff_time:
                        new_papers.append(paper)
                else:
                    # å¦‚æœè®¾ç½®äº†ä¸Šæ¬¡æ£€æŸ¥æ—¶é—´ï¼Œåªè·å–è¯¥æ—¶é—´ä¹‹åçš„è®ºæ–‡
                    if self.config["last_check"]:
                        try:
                            last_check = datetime.fromisoformat(self.config["last_check"])
                            if paper['published'].replace(tzinfo=None) > last_check.replace(tzinfo=None):
                                new_papers.append(paper)
                        except:
                            # å¦‚æœæ—¥æœŸè§£æå¤±è´¥ï¼Œå°±æ·»åŠ è¿™ç¯‡è®ºæ–‡
                            new_papers.append(paper)
                    else:
                        new_papers.append(paper)
        
        return new_papers
    
    def send_notification(self, title, message):
        """å‘é€ç³»ç»Ÿé€šçŸ¥"""
        try:
            notification.notify(
                title=title,
                message=message,
                timeout=10
            )
        except Exception as e:
            self.logger.error(f"å‘é€é€šçŸ¥å¤±è´¥: {e}")
            # å¦‚æœé€šçŸ¥å¤±è´¥ï¼Œè‡³å°‘åœ¨æ§åˆ¶å°æ˜¾ç¤º
            print(f"\nğŸ”” {title}: {message}")
    
    def test_arxiv_connection(self):
        """æµ‹è¯•arxivè¿æ¥å’ŒAPI"""
        print("ğŸ” æ­£åœ¨æµ‹è¯•arXivè¿æ¥...")
        
        try:
            # æµ‹è¯•ç›´æ¥APIè°ƒç”¨
            test_query = "cat:cs.AI"
            papers = self.search_papers_direct_api(test_query, max_results=2)
            
            if papers:
                print(f"âœ… arXiv APIè¿æ¥æˆåŠŸï¼è·å–åˆ° {len(papers)} ç¯‡è®ºæ–‡")
                print(f"ğŸ“„ æµ‹è¯•è®ºæ–‡: {papers[0]['title'][:80]}...")
                print(f"ğŸ‘¥ ä½œè€…: {', '.join(papers[0]['authors'][:2])}")
                print(f"ğŸ“… å‘å¸ƒ: {papers[0]['published'].strftime('%Y-%m-%d')}")
                return True
            else:
                print("âŒ arXiv APIè¿æ¥å¤±è´¥")
                return False
                
        except Exception as e:
            print(f"âŒ arXivè¿æ¥æµ‹è¯•å¤±è´¥: {e}")
            return False
    
    def check_for_new_papers(self):
        """æ£€æŸ¥æ–°è®ºæ–‡"""
        self.logger.info("å¼€å§‹æ£€æŸ¥æ–°è®ºæ–‡...")
        self.create_download_directory()
        
        # å¦‚æœæ˜¯é¦–æ¬¡è¿è¡Œï¼Œæç¤ºç”¨æˆ·
        if self.config.get("first_run", True):
            print("ğŸ“¢ è¿™æ˜¯é¦–æ¬¡è¿è¡Œï¼Œå°†ä¸‹è½½æœ€è¿‘24å°æ—¶çš„è®ºæ–‡ä½œä¸ºæ¼”ç¤º")
        
        all_new_papers = []
        
        # å¯¹æ¯ä¸ªæœç´¢æŸ¥è¯¢è¿›è¡Œæ£€æŸ¥
        for query in self.config["search_queries"]:
            self.logger.info(f"æœç´¢æŸ¥è¯¢: {query}")
            papers = self.search_papers(query, self.config["max_results"])
            
            if papers:
                self.logger.info(f"æŸ¥è¯¢ '{query}' æ‰¾åˆ° {len(papers)} ç¯‡è®ºæ–‡")
                new_papers = self.filter_new_papers(papers)
                
                if new_papers:
                    self.logger.info(f"å…¶ä¸­ {len(new_papers)} ç¯‡æ˜¯æ–°è®ºæ–‡")
                    all_new_papers.extend(new_papers)
                else:
                    self.logger.info("æ²¡æœ‰æ–°è®ºæ–‡")
            else:
                self.logger.warning(f"æŸ¥è¯¢ '{query}' æ²¡æœ‰è¿”å›ç»“æœ")
        
        # å»é‡ï¼ˆåŸºäºIDï¼‰
        unique_papers = {}
        for paper in all_new_papers:
            if paper['id'] not in unique_papers:
                unique_papers[paper['id']] = paper
        all_new_papers = list(unique_papers.values())
        
        if all_new_papers:
            print(f"\nğŸ¯ æ‰¾åˆ° {len(all_new_papers)} ç¯‡æ–°è®ºæ–‡ï¼Œå¼€å§‹ä¸‹è½½...")
        
        # ä¸‹è½½æ–°è®ºæ–‡
        successful_downloads = 0
        for i, paper in enumerate(all_new_papers, 1):
            print(f"ğŸ“¥ ä¸‹è½½è¿›åº¦: {i}/{len(all_new_papers)} - {paper['title'][:50]}...")
            if self.download_paper(paper):
                self.config["downloaded_papers"].append(paper['id'])
                successful_downloads += 1
        
        # æ ‡è®°é¦–æ¬¡è¿è¡Œå·²å®Œæˆ
        if self.config.get("first_run", True):
            self.config["first_run"] = False
        
        # æ›´æ–°é…ç½®
        self.config["last_check"] = datetime.now().isoformat()
        self.save_config()
        
        # å‘é€é€šçŸ¥
        if successful_downloads > 0:
            message = f"æˆåŠŸä¸‹è½½äº† {successful_downloads} ç¯‡æ–°è®ºæ–‡!"
            self.send_notification("arXivè®ºæ–‡æ›´æ–°", message)
            self.logger.info(message)
            
            # æ‰“å°è®ºæ–‡ä¿¡æ¯
            print("\n" + "="*60)
            print("ğŸ‰ æ–°ä¸‹è½½çš„è®ºæ–‡")
            print("="*60)
            for paper in all_new_papers:
                if paper['id'] in self.config["downloaded_papers"]:
                    print(f"\nğŸ“„ æ ‡é¢˜: {paper['title']}")
                    print(f"ğŸ†” ID: {paper['id']}")
                    print(f"ğŸ‘¥ ä½œè€…: {', '.join(paper['authors'][:3])}" + 
                          (f" ç­‰ {len(paper['authors'])} äºº" if len(paper['authors']) > 3 else ""))
                    print(f"ğŸ·ï¸  ç±»åˆ«: {', '.join(paper['categories']) if paper['categories'] else 'æœªçŸ¥'}")
                    print(f"ğŸ“… å‘å¸ƒ: {paper['published'].strftime('%Y-%m-%d %H:%M') if hasattr(paper['published'], 'strftime') else paper['published']}")
                    print(f"ğŸ“ æ‘˜è¦: {paper['summary'][:200]}...")
                    print(f"ğŸ”— é“¾æ¥: {paper['pdf_url']}")
                    print("-" * 60)
        else:
            self.logger.info("æ²¡æœ‰æ‰¾åˆ°æ–°è®ºæ–‡")
            print("â„¹ï¸  æ²¡æœ‰æ‰¾åˆ°æ–°è®ºæ–‡")
    
    def reset_downloaded_papers(self):
        """é‡ç½®ä¸‹è½½è®°å½•"""
        self.config["downloaded_papers"] = []
        self.config["last_check"] = None
        self.config["first_run"] = True
        self.save_config()
        print("âœ… å·²é‡ç½®ä¸‹è½½è®°å½•ï¼Œä¸‹æ¬¡æ£€æŸ¥æ—¶å°†é‡æ–°ä¸‹è½½è®ºæ–‡")
    
    def add_search_query(self, query):
        """æ·»åŠ æœç´¢æŸ¥è¯¢"""
        if query not in self.config["search_queries"]:
            self.config["search_queries"].append(query)
            self.save_config()
            self.logger.info(f"æ·»åŠ æœç´¢æŸ¥è¯¢: {query}")
    
    def remove_search_query(self, query):
        """ç§»é™¤æœç´¢æŸ¥è¯¢"""
        if query in self.config["search_queries"]:
            self.config["search_queries"].remove(query)
            self.save_config()
            self.logger.info(f"ç§»é™¤æœç´¢æŸ¥è¯¢: {query}")
    
    def start_monitoring(self):
        """å¼€å§‹ç›‘æ§"""
        self.logger.info("å¼€å§‹arXivè®ºæ–‡ç›‘æ§...")
        print("ğŸš€ å¯åŠ¨arXivè®ºæ–‡ç›‘æ§å™¨...")
        
        # ç«‹å³æ‰§è¡Œä¸€æ¬¡æ£€æŸ¥
        self.check_for_new_papers()
        
        # è®¾ç½®å®šæœŸæ£€æŸ¥
        schedule.every(self.config["check_interval_hours"]).hours.do(self.check_for_new_papers)
        
        print(f"â° ç›‘æ§å·²å¯åŠ¨ï¼Œæ¯ {self.config['check_interval_hours']} å°æ—¶æ£€æŸ¥ä¸€æ¬¡")
        print("ğŸ’¡ æŒ‰ Ctrl+C åœæ­¢ç›‘æ§")
        
        try:
            while True:
                schedule.run_pending()
                time.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡æ˜¯å¦æœ‰å¾…æ‰§è¡Œçš„ä»»åŠ¡
        except KeyboardInterrupt:
            self.logger.info("ç›‘æ§å·²åœæ­¢")
            print("\nğŸ›‘ ç›‘æ§å·²åœæ­¢")

def main():
    """ä¸»å‡½æ•°"""
    print("="*60)
    print("ğŸ“š arXivè®ºæ–‡ç›‘æ§å™¨ v2.0")
    print("="*60)
    
    monitor = ArxivMonitor()
    
    while True:
        print("\nğŸ¯ è¯·é€‰æ‹©æ“ä½œ:")
        print("1. ğŸš€ å¼€å§‹ç›‘æ§")
        print("2. ğŸ” æ‰‹åŠ¨æ£€æŸ¥ä¸€æ¬¡")
        print("3. ğŸ“‹ æŸ¥çœ‹å½“å‰æœç´¢ä¸»é¢˜")
        print("4. â• æ·»åŠ æœç´¢ä¸»é¢˜")
        print("5. â– åˆ é™¤æœç´¢ä¸»é¢˜")
        print("6. â±ï¸  è®¾ç½®æ£€æŸ¥é—´éš”")
        print("7. ğŸ“Š æŸ¥çœ‹ç»Ÿè®¡ä¿¡æ¯")
        print("8. ğŸ§ª æµ‹è¯•arXivè¿æ¥")
        print("9. ğŸ”„ é‡ç½®ä¸‹è½½è®°å½•")
        print("0. ğŸšª é€€å‡º")
        
        choice = input("\nè¯·è¾“å…¥é€‰æ‹© (0-9): ").strip()
        
        if choice == '1':
            monitor.start_monitoring()
        
        elif choice == '2':
            print("ğŸ” æ­£åœ¨æ£€æŸ¥æ–°è®ºæ–‡...")
            monitor.check_for_new_papers()
        
        elif choice == '3':
            print(f"\nğŸ“‹ å½“å‰æœç´¢ä¸»é¢˜ (å…±{len(monitor.config['search_queries'])}ä¸ª):")
            for i, query in enumerate(monitor.config["search_queries"], 1):
                print(f"   {i}. {query}")
        
        elif choice == '4':
            print("\nğŸ’¡ å¸¸ç”¨æœç´¢ä¸»é¢˜ç¤ºä¾‹:")
            examples = [
                ("cat:cs.AI", "äººå·¥æ™ºèƒ½"),
                ("cat:cs.LG", "æœºå™¨å­¦ä¹ "),
                ("cat:cs.CV", "è®¡ç®—æœºè§†è§‰"),
                ("cat:cs.CL", "è®¡ç®—è¯­è¨€å­¦/NLP"),
                ("cat:cs.RO", "æœºå™¨äººå­¦"),
                ("cat:stat.ML", "ç»Ÿè®¡æœºå™¨å­¦ä¹ "),
                ("deep learning", "å…³é”®è¯: æ·±åº¦å­¦ä¹ "),
                ("neural network", "å…³é”®è¯: ç¥ç»ç½‘ç»œ"),
                ("transformer", "å…³é”®è¯: Transformer"),
                ("large language model", "å…³é”®è¯: å¤§è¯­è¨€æ¨¡å‹"),
                ("diffusion model", "å…³é”®è¯: æ‰©æ•£æ¨¡å‹"),
                ("reinforcement learning", "å…³é”®è¯: å¼ºåŒ–å­¦ä¹ ")
            ]
            
            for query, desc in examples:
                print(f"   â€¢ {query:<25} ({desc})")
            
            query = input("\nè¯·è¾“å…¥è¦æ·»åŠ çš„æœç´¢ä¸»é¢˜: ").strip()
            if query:
                monitor.add_search_query(query)
                print(f"âœ… å·²æ·»åŠ æœç´¢ä¸»é¢˜: {query}")
        
        elif choice == '5':
            if not monitor.config["search_queries"]:
                print("âŒ æ²¡æœ‰æœç´¢ä¸»é¢˜å¯åˆ é™¤")
                continue
                
            print(f"\nğŸ“‹ å½“å‰æœç´¢ä¸»é¢˜:")
            for i, query in enumerate(monitor.config["search_queries"], 1):
                print(f"   {i}. {query}")
            
            try:
                index = int(input("\nè¯·è¾“å…¥è¦åˆ é™¤çš„ä¸»é¢˜ç¼–å·: ")) - 1
                if 0 <= index < len(monitor.config["search_queries"]):
                    removed_query = monitor.config["search_queries"].pop(index)
                    monitor.save_config()
                    print(f"âœ… å·²åˆ é™¤æœç´¢ä¸»é¢˜: {removed_query}")
                else:
                    print("âŒ æ— æ•ˆçš„ç¼–å·")
            except ValueError:
                print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—")
        
        elif choice == '6':
            try:
                current = monitor.config['check_interval_hours']
                hours = int(input(f"è¯·è¾“å…¥æ£€æŸ¥é—´éš”(å°æ—¶ï¼Œå½“å‰: {current}): "))
                if hours > 0:
                    monitor.config["check_interval_hours"] = hours
                    monitor.save_config()
                    print(f"âœ… æ£€æŸ¥é—´éš”å·²è®¾ç½®ä¸º {hours} å°æ—¶")
                else:
                    print("âŒ è¯·è¾“å…¥å¤§äº0çš„æ•°å­—")
            except ValueError:
                print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—")
        
        elif choice == '7':
            print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
            print(f"   â€¢ æœç´¢ä¸»é¢˜æ•°é‡: {len(monitor.config['search_queries'])}")
            print(f"   â€¢ å·²ä¸‹è½½è®ºæ–‡æ•°: {len(monitor.config['downloaded_papers'])}")
            print(f"   â€¢ ä¸‹è½½è·¯å¾„: {monitor.config['download_path']}")
            print(f"   â€¢ æ£€æŸ¥é—´éš”: {monitor.config['check_interval_hours']} å°æ—¶")
            print(f"   â€¢ é¦–æ¬¡è¿è¡Œ: {'æ˜¯' if monitor.config.get('first_run', True) else 'å¦'}")
            if monitor.config['last_check']:
                print(f"   â€¢ ä¸Šæ¬¡æ£€æŸ¥: {monitor.config['last_check'][:19]}")
            else:
                print(f"   â€¢ ä¸Šæ¬¡æ£€æŸ¥: ä»æœªæ£€æŸ¥")
            
            # æ˜¾ç¤ºä¸‹è½½ç›®å½•å¤§å°
            if os.path.exists(monitor.config['download_path']):
                total_size = 0
                file_count = 0
                for root, dirs, files in os.walk(monitor.config['download_path']):
                    for file in files:
                        if file.endswith('.pdf'):
                            file_count += 1
                            filepath = os.path.join(root, file)
                            total_size += os.path.getsize(filepath)
                
                print(f"   â€¢ æ–‡ä»¶æ€»æ•°: {file_count} ä¸ªPDFæ–‡ä»¶")
                print(f"   â€¢ æ€»å¤§å°: {total_size / 1024 / 1024:.1f} MB")
        
        elif choice == '8':
            monitor.test_arxiv_connection()
        
        elif choice == '9':
            confirm = input("âš ï¸  ç¡®å®šè¦é‡ç½®ä¸‹è½½è®°å½•å—ï¼Ÿè¿™å°†å¯¼è‡´ä¸‹æ¬¡æ£€æŸ¥æ—¶é‡æ–°ä¸‹è½½è®ºæ–‡ (y/N): ")
            if confirm.lower() in ['y', 'yes']:
                monitor.reset_downloaded_papers()
            else:
                print("âŒ æ“ä½œå·²å–æ¶ˆ")
        
        elif choice == '0':
            print("ğŸ‘‹ å†è§!")
            break
        
        else:
            print("âŒ æ— æ•ˆçš„é€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥")

if __name__ == "__main__":
    main()